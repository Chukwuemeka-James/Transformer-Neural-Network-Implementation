{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO5Z0qErNuOt"
      },
      "source": [
        "# Self Attention in Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HedntyUvLrBo"
      },
      "source": [
        "## Generate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xtKbaWhFJui3"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "L, d_k, d_v = 4, 8, 8\n",
        "q = np.random.randn(L, d_k)\n",
        "k = np.random.randn(L, d_k)\n",
        "v = np.random.randn(L, d_v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09JpvuNJ2sZC",
        "outputId": "30d2c627-8647-44e0-aa92-c9e53e3b7843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q\n",
            " [[-0.50598333  0.59287364 -0.64212808 -0.17667384  1.78683356 -0.03810015\n",
            "  -2.79906259 -0.87436271]\n",
            " [-1.1635331   0.39761587  0.7502223  -0.19290089 -0.27755457 -0.7598107\n",
            "   0.88142972 -0.44349095]\n",
            " [-0.28506127 -2.0400416  -1.96026798 -0.63174484  0.49923543 -0.57036319\n",
            "   1.49095731  0.89803188]\n",
            " [-1.62443852  1.453917   -1.49314297  0.95953202 -1.60815407  0.1632695\n",
            "  -0.98389182 -1.1222311 ]]\n",
            "K\n",
            " [[-1.58265754  0.76202182  2.32828504  0.50024378 -0.38254118  2.16603257\n",
            "  -0.53065346 -0.19447538]\n",
            " [ 0.59827253  0.12909976  0.10836157 -0.15652026 -0.33419576  0.25269797\n",
            "   0.76630028  0.37939898]\n",
            " [-2.36337659 -0.03495578  1.25475903  0.10300347  1.11828814  0.16100326\n",
            "   0.49260405  0.29632038]\n",
            " [-0.05618466 -0.58816675  0.46506258 -0.04566082  1.33489973 -0.43446506\n",
            "   0.37468388 -0.33398102]]\n",
            "V\n",
            " [[-2.33028699e+00  4.18506262e-01  1.57360937e+00  1.37308738e-01\n",
            "   8.52094837e-01  6.58167848e-01 -4.12112209e-01 -2.52690046e-03]\n",
            " [-2.40477900e+00 -8.15979439e-01 -8.21925214e-01  1.41383638e-01\n",
            "  -4.26737879e-01 -1.62771803e+00  5.56628997e-01 -9.50161423e-01]\n",
            " [ 5.69531565e-01  1.16653634e+00 -7.47385138e-01 -6.12570059e-01\n",
            "   4.95602990e-01  1.32665094e-01 -6.74559168e-01 -3.76896324e-01]\n",
            " [ 9.61138903e-01  1.44294896e-01  3.01901776e-01  3.20771720e+00\n",
            "  -6.38903868e-01 -5.71129671e-01 -5.16128064e-01 -1.32604188e+00]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV6txskBLwjh"
      },
      "source": [
        "## Self Attention\n",
        "\n",
        "$$\n",
        "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{new V} = \\text{self attention}.V\n",
        "$$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7GePHKk3Mh0",
        "outputId": "7dae7f5e-4715-4fd4-fbfd-7c0815e7d39e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.5584545 , -3.35153984,  0.70533024,  1.03421098],\n",
              "       [ 1.87361595, -0.12535484,  3.52750914,  0.62719536],\n",
              "       [-8.37572099,  0.62481134, -0.31270199,  1.5060436 ],\n",
              "       [ 2.39156799, -1.69717588, -0.57566407, -3.71359891]])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.matmul(q, k.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The line `np.matmul(q, k.T)` is a **crucial step** in the **self-attention mechanism** of Transformers, specifically in the computation of the **attention scores**. \n",
        "\n",
        "### **What is Self-Attention?**\n",
        "Self-attention helps a model focus on the most relevant parts of the input sequence for each token when processing sequences. It allows the model to compute relationships between tokens and dynamically weigh their importance.\n",
        "\n",
        "Key components:\n",
        "- **Query (q):** What the current token is \"looking for.\"\n",
        "- **Key (k):** Represents the \"content\" of all tokens in the sequence.\n",
        "- **Value (v):** Holds the information to be extracted and passed along.\n",
        "\n",
        "`np.matmul(q, k.T)`:\n",
        "- Calculates similarity scores between query (`q`) and key (`k`) vectors.\n",
        "- Encodes relationships between tokens in the input sequence.\n",
        "- Forms the foundation of the self-attention mechanism. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odK76OoI3nL2",
        "outputId": "69b50cdb-9a41-45ae-bfd2-619228af1ef7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1.1902732818147692, 0.7955736609453732, 7.7092653269714475)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Why we need sqrt(d_k) in denominator\n",
        "q.var(), k.var(), np.matmul(q, k.T).var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ps6AY1Q3tRI",
        "outputId": "3b9ac3c8-70b8-47bd-e868-e7d6fd26d270"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1.1902732818147692, 0.7955736609453732, 0.9636581658714307)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
        "q.var(), k.var(), scaled.var()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypO9IK1PL3cJ"
      },
      "source": [
        "Notice the reduction in variance of the product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVHAJR4N4VQX",
        "outputId": "52b06cf8-0381-453c-b576-0bd8de9a38b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.19744348, -1.18494828,  0.2493719 ,  0.3656488 ],\n",
              "       [ 0.66242327, -0.04431963,  1.24716282,  0.22174705],\n",
              "       [-2.96126456,  0.22090417, -0.11055685,  0.53246682],\n",
              "       [ 0.84554697, -0.60004229, -0.20352798, -1.31295549]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmz4v-RmMAaj"
      },
      "source": [
        "## Masking\n",
        "\n",
        "- This is to ensure words don't get context from words generated in the future. \n",
        "- Not required in the encoders, but required int he decoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8N3OhSLILfG",
        "outputId": "2c63a444-066c-44b2-abe5-242dd989f311"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [1., 1., 0., 0.],\n",
              "       [1., 1., 1., 0.],\n",
              "       [1., 1., 1., 1.]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask = np.tril(np.ones( (L, L) ))\n",
        "mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hIV9K3Yn6s1V"
      },
      "outputs": [],
      "source": [
        "mask[mask == 0] = -np.infty\n",
        "mask[mask == 1] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK5V_T3W6vpX",
        "outputId": "bb4160a1-a011-4850-e403-9cb252572c66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0., -inf, -inf, -inf],\n",
              "       [  0.,   0., -inf, -inf],\n",
              "       [  0.,   0.,   0., -inf],\n",
              "       [  0.,   0.,   0.,   0.]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNH1VgEf7xTa",
        "outputId": "4211c411-0356-4e39-8388-d39b0c1d0920"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.19744348,        -inf,        -inf,        -inf],\n",
              "       [ 0.66242327, -0.04431963,        -inf,        -inf],\n",
              "       [-2.96126456,  0.22090417, -0.11055685,        -inf],\n",
              "       [ 0.84554697, -0.60004229, -0.20352798, -1.31295549]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaled + mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMTAXjooN9eZ"
      },
      "source": [
        "## Softmax\n",
        "\n",
        "$$\n",
        "\\text{softmax} = \\frac{e^{x_i}}{\\sum_j e^x_j}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2R4gdRqj8W4Y"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "K5eg2zPy41sP"
      },
      "outputs": [],
      "source": [
        "attention = softmax(scaled + mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sauNmfl-1TB",
        "outputId": "46b22beb-9034-4c7c-8d56-04209d2581c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.        , 0.        , 0.        , 0.        ],\n",
              "       [0.66968106, 0.33031894, 0.        , 0.        ],\n",
              "       [0.02358547, 0.56838537, 0.40802916, 0.        ],\n",
              "       [0.58776275, 0.13848114, 0.20587072, 0.06788539]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAy37go56LZo",
        "outputId": "78d97fa1-e0b3-4c1d-8294-bf0fdb77f199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-2.33028699,  0.41850626,  1.57360937,  0.13730874,  0.85209484,\n",
              "         0.65816785, -0.41211221, -0.0025269 ],\n",
              "       [-2.35489311,  0.01073226,  0.78231893,  0.13865475,  0.42967217,\n",
              "        -0.09690355, -0.09211864, -0.31554853],\n",
              "       [-1.18941663,  0.02206074, -0.73501088, -0.16634756, -0.02023404,\n",
              "        -0.85551669,  0.03142011, -0.69390214],\n",
              "       [-1.52017524,  0.38293583,  0.67771783,  0.19193084,  0.50039236,\n",
              "         0.14997879, -0.33905113, -0.30067545]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_v = np.matmul(attention, v)\n",
        "new_v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCP2aZOU9VrT",
        "outputId": "e1fe2137-cd95-4a4b-fa1a-3ec21c38104c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-2.33028699e+00,  4.18506262e-01,  1.57360937e+00,\n",
              "         1.37308738e-01,  8.52094837e-01,  6.58167848e-01,\n",
              "        -4.12112209e-01, -2.52690046e-03],\n",
              "       [-2.40477900e+00, -8.15979439e-01, -8.21925214e-01,\n",
              "         1.41383638e-01, -4.26737879e-01, -1.62771803e+00,\n",
              "         5.56628997e-01, -9.50161423e-01],\n",
              "       [ 5.69531565e-01,  1.16653634e+00, -7.47385138e-01,\n",
              "        -6.12570059e-01,  4.95602990e-01,  1.32665094e-01,\n",
              "        -6.74559168e-01, -3.76896324e-01],\n",
              "       [ 9.61138903e-01,  1.44294896e-01,  3.01901776e-01,\n",
              "         3.20771720e+00, -6.38903868e-01, -5.71129671e-01,\n",
              "        -5.16128064e-01, -1.32604188e+00]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSiJuBQELFHT"
      },
      "source": [
        "# Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XvTnmdcB_jdq"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "  d_k = q.shape[-1]\n",
        "  scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scaled = scaled + mask\n",
        "  attention = softmax(scaled)\n",
        "  out = np.matmul(attention, v)\n",
        "  return out, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSxLkZdiSLMT",
        "outputId": "ca70508d-fb6e-4eec-acb6-7a89a60dffa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q\n",
            " [[-0.50598333  0.59287364 -0.64212808 -0.17667384  1.78683356 -0.03810015\n",
            "  -2.79906259 -0.87436271]\n",
            " [-1.1635331   0.39761587  0.7502223  -0.19290089 -0.27755457 -0.7598107\n",
            "   0.88142972 -0.44349095]\n",
            " [-0.28506127 -2.0400416  -1.96026798 -0.63174484  0.49923543 -0.57036319\n",
            "   1.49095731  0.89803188]\n",
            " [-1.62443852  1.453917   -1.49314297  0.95953202 -1.60815407  0.1632695\n",
            "  -0.98389182 -1.1222311 ]]\n",
            "K\n",
            " [[-1.58265754  0.76202182  2.32828504  0.50024378 -0.38254118  2.16603257\n",
            "  -0.53065346 -0.19447538]\n",
            " [ 0.59827253  0.12909976  0.10836157 -0.15652026 -0.33419576  0.25269797\n",
            "   0.76630028  0.37939898]\n",
            " [-2.36337659 -0.03495578  1.25475903  0.10300347  1.11828814  0.16100326\n",
            "   0.49260405  0.29632038]\n",
            " [-0.05618466 -0.58816675  0.46506258 -0.04566082  1.33489973 -0.43446506\n",
            "   0.37468388 -0.33398102]]\n",
            "V\n",
            " [[-2.33028699e+00  4.18506262e-01  1.57360937e+00  1.37308738e-01\n",
            "   8.52094837e-01  6.58167848e-01 -4.12112209e-01 -2.52690046e-03]\n",
            " [-2.40477900e+00 -8.15979439e-01 -8.21925214e-01  1.41383638e-01\n",
            "  -4.26737879e-01 -1.62771803e+00  5.56628997e-01 -9.50161423e-01]\n",
            " [ 5.69531565e-01  1.16653634e+00 -7.47385138e-01 -6.12570059e-01\n",
            "   4.95602990e-01  1.32665094e-01 -6.74559168e-01 -3.76896324e-01]\n",
            " [ 9.61138903e-01  1.44294896e-01  3.01901776e-01  3.20771720e+00\n",
            "  -6.38903868e-01 -5.71129671e-01 -5.16128064e-01 -1.32604188e+00]]\n",
            "New V\n",
            " [[-2.33028699  0.41850626  1.57360937  0.13730874  0.85209484  0.65816785\n",
            "  -0.41211221 -0.0025269 ]\n",
            " [-2.35489311  0.01073226  0.78231893  0.13865475  0.42967217 -0.09690355\n",
            "  -0.09211864 -0.31554853]\n",
            " [-1.18941663  0.02206074 -0.73501088 -0.16634756 -0.02023404 -0.85551669\n",
            "   0.03142011 -0.69390214]\n",
            " [-1.52017524  0.38293583  0.67771783  0.19193084  0.50039236  0.14997879\n",
            "  -0.33905113 -0.30067545]]\n",
            "Attention\n",
            " [[1.         0.         0.         0.        ]\n",
            " [0.66968106 0.33031894 0.         0.        ]\n",
            " [0.02358547 0.56838537 0.40802916 0.        ]\n",
            " [0.58776275 0.13848114 0.20587072 0.06788539]]\n"
          ]
        }
      ],
      "source": [
        "values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)\n",
        "print(\"New V\\n\", values)\n",
        "print(\"Attention\\n\", attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HtQQtB2LJus"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "TF_PT_Rec_System",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
