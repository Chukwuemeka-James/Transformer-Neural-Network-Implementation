{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UdTblxTF_byq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Explanation of Transformer Encoding**\n",
        "\n",
        "The code below provides a modular implementation of the Transformer Encoder. Below are the key components and their roles: \n",
        "\n",
        "#### **1. Scaled Dot-Product Attention**\n",
        "- **Code:** `scaled_dot_product(q, k, v, mask=None)`\n",
        "- This function computes attention scores by performing:\n",
        "  1. **Dot product** between query (\\(q\\)) and key (\\(k\\)) vectors, scaled by the square root of their dimensionality (\\(d_k\\)).\n",
        "  2. **Masking (optional):** Masks certain positions (e.g., padding or future tokens).\n",
        "  3. **Softmax:** Converts the scores into probabilities for attention weights.\n",
        "  4. **Weighted sum:** Applies these weights to the value (\\(v\\)) vectors to compute the final output.\n",
        "- The attention mechanism allows the model to focus on relevant parts of the input sequence.\n",
        "\n",
        "#### **2. Multi-Head Attention**\n",
        "- **Code:** `MultiHeadAttention(nn.Module)`\n",
        "- Breaks the input into multiple \"heads,\" each attending to different parts of the sequence. \n",
        "- Key steps:\n",
        "  1. **Projection:** Maps input features (\\(x\\)) into query, key, and value matrices using a shared linear layer.\n",
        "  2. **Reshape and Permutation:** Prepares the tensors for multi-head processing by reshaping and reordering dimensions.\n",
        "  3. **Attention Calculation:** Computes scaled dot-product attention for each head.\n",
        "  4. **Aggregation:** Combines the outputs of all heads and applies a final linear transformation.\n",
        "- Multi-head attention enhances the modelâ€™s ability to capture diverse relationships in the data.\n",
        "\n",
        "#### **3. Layer Normalization**\n",
        "- **Code:** `LayerNormalization(nn.Module)`\n",
        "- Applied after attention and feedforward layers to stabilize training and improve convergence. \n",
        "- Normalizes activations across the feature dimension using:\n",
        "  \\[\n",
        "  \\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
        "  \\]\n",
        "- Maintains learnable parameters (\\(\\gamma\\), \\(\\beta\\)) for scaling and shifting.\n",
        "\n",
        "#### **4. Position-Wise Feedforward Network**\n",
        "- **Code:** `PositionwiseFeedForward(nn.Module)`\n",
        "- Consists of two fully connected layers with a non-linearity (\\(ReLU\\)) and dropout in between.\n",
        "- Applies transformations independently at each sequence position to introduce non-linearity and enrich the representation.\n",
        "\n",
        "#### **5. Encoder Layer**\n",
        "- **Code:** `EncoderLayer(nn.Module)`\n",
        "- Combines all components into a single layer:\n",
        "  1. Multi-head attention.\n",
        "  2. Residual connection and LayerNorm.\n",
        "  3. Feedforward network.\n",
        "  4. Another residual connection and LayerNorm.\n",
        "- Adds dropout for regularization at appropriate stages.\n",
        "\n",
        "#### **6. Encoder**\n",
        "- **Code:** `Encoder(nn.Module)`\n",
        "- Stacks multiple encoder layers sequentially to form the full Transformer Encoder.\n",
        "- Each layer refines the representation, allowing the model to build a deeper understanding of the input.\n",
        "\n",
        "### **Workflow**\n",
        "1. Input data (\\(x\\)) passes through multiple layers of the encoder.\n",
        "2. Each layer applies multi-head attention, feedforward networks, and normalization to process the sequence.\n",
        "3. The output is a refined representation capturing both local and global dependencies.\n",
        "\n",
        "This implementation modularizes the Transformer Encoder, providing flexibility for modification or debugging through its well-structured components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mYbIgWZ3_JXg"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k) # 30 x 8 x 200 x 200\n",
        "    print(f\"scaled.size() : {scaled.size()}\")\n",
        "    if mask is not None:\n",
        "        print(f\"-- ADDING MASK of shape {mask.size()} --\") \n",
        "        # Broadcasting add. So just the last N dimensions need to match\n",
        "        scaled += mask\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model # 512\n",
        "        self.num_heads = num_heads # 8\n",
        "        self.head_dim = d_model // num_heads # 64 = 512/8\n",
        "        self.qkv_layer = nn.Linear(d_model , 3 * d_model) # 512*1536\n",
        "        self.linear_layer = nn.Linear(d_model, d_model) # 512 *512\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, max_sequence_length, d_model = x.size() # 30 x 200 x 512 \n",
        "        print(f\"x.size(): {x.size()}\")\n",
        "        qkv = self.qkv_layer(x) # 30 x 200 x 512 \n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        qkv = qkv.reshape(batch_size, max_sequence_length, self.num_heads, 3 * self.head_dim) # 30 x 200 x 8 x 192\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        qkv = qkv.permute(0, 2, 1, 3) # 30 x 8 200x 192\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        q, k, v = qkv.chunk(3, dim=-1) # each are 30 x 8 x 200 x 64 \n",
        "        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\n",
        "        values = values.reshape(batch_size, max_sequence_length, self.num_heads * self.head_dim)\n",
        "        print(f\"values.size(): {values.size()}\")\n",
        "        out = self.linear_layer(values)\n",
        "        print(f\"out.size(): {out.size()}\")\n",
        "        return out\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.parameters_shape=parameters_shape # 512\n",
        "        self.eps=eps\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape)) # 512\n",
        "        self.beta =  nn.Parameter(torch.zeros(parameters_shape)) # 512\n",
        "\n",
        "    def forward(self, inputs): # 30 x 200 x 512\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))] # -1\n",
        "        mean = inputs.mean(dim=dims, keepdim=True) # 30 x 200 x 1\n",
        "        print(f\"Mean ({mean.size()})\")\n",
        "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True) # 30 x 200 x 1\n",
        "        std = (var + self.eps).sqrt() # 20 x 200 x 1\n",
        "        print(f\"Standard Deviation  ({std.size()})\")\n",
        "        y = (inputs - mean) / std # 30 x 200 x 512\n",
        "        print(f\"y: {y.size()}\")\n",
        "        out = self.gamma * y  + self.beta # 30 x 200 x 512 for both\n",
        "        print(f\"self.gamma: {self.gamma.size()}, self.beta: {self.beta.size()}\")\n",
        "        print(f\"out: {out.size()}\")\n",
        "        return out\n",
        "\n",
        "  \n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden) # 512 x 2048\n",
        "        self.linear2 = nn.Linear(hidden, d_model) # 2048 x 512\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x): # 30 x 200 x 512\n",
        "        x = self.linear1(x) # 30 x 200 x 2048\n",
        "        print(f\"x after first linear layer: {x.size()}\")\n",
        "        x = self.relu(x) # 30 x 200 x 2048\n",
        "        print(f\"x after activation: {x.size()}\")\n",
        "        x = self.dropout(x) # 30 x 200 x 2048\n",
        "        print(f\"x after dropout: {x.size()}\")\n",
        "        x = self.linear2(x) # 30 x 200 x 512\n",
        "        print(f\"x after 2nd linear layer: {x.size()}\")\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual_x = x # 30 x 200 x 512\n",
        "        print(\"------- ATTENTION 1 ------\")\n",
        "        x = self.attention(x, mask=None) # 30 x 200 x 512\n",
        "        print(\"------- DROPOUT 1 ------\")\n",
        "        x = self.dropout1(x) # 30 x 200 x 512\n",
        "        print(\"------- ADD AND LAYER NORMALIZATION 1 ------\")\n",
        "        x = self.norm1(x + residual_x) # 30 x 200 x 512\n",
        "        residual_x = x # 30 x 200 x 512\n",
        "        print(\"------- ATTENTION 2 ------\")\n",
        "        x = self.ffn(x) # 30 x 200 x 512\n",
        "        print(\"------- DROPOUT 2 ------\")\n",
        "        x = self.dropout2(x) # 30 x 200 x 512\n",
        "        print(\"------- ADD AND LAYER NORMALIZATION 2 ------\")\n",
        "        x = self.norm2(x + residual_x) # 30 x 200 x 512\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
        "        super().__init__()\n",
        "        # Create a sequential stack of 'num_layers' EncoderLayer modules with the specified parameters.\n",
        "        self.layers = nn.Sequential(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
        "                                     for _ in range(num_layers)])\n",
        "        \n",
        "    # Pass the input through the stacked encoder layers sequentially and return the output. \n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0FzFhz5N_5qd"
      },
      "outputs": [],
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "batch_size = 30\n",
        "max_sequence_length = 200\n",
        "ffn_hidden = 2048 # Feed forward network hiddern\n",
        "num_layers = 5 \n",
        "\n",
        "encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. **`d_model`**: The dimensionality of the input and output features in the model.  \n",
        "2. **`num_heads`**: The number of attention heads in the multi-head attention mechanism.  \n",
        "3. **`drop_prob`**: The probability of dropping elements during dropout for regularization.  \n",
        "4. **`batch_size`**: The number of sequences processed together in a single forward and backward pass.  \n",
        "5. **`max_sequence_length`**: The maximum number of tokens in each input sequence.  \n",
        "6. **`ffn_hidden`**: The number of hidden units in the feedforward network within each encoder layer.  \n",
        "7. **`num_layers`**: The number of encoder layers stacked in the Transformer encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysrs-8UlADVd",
        "outputId": "505ca0b2-ac1f-4b52-f491-328ef2bf40b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------- ATTENTION 1 ------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 1 ------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 1 ------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 1 ------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 1 ------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "out.size(): torch.Size([30, 200, 512])\n",
            "------- DROPOUT 1 ------\n",
            "------- ADD AND LAYER NORMALIZATION 1 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "------- ATTENTION 2 ------\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after activation: torch.Size([30, 200, 2048])\n",
            "x after dropout: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "------- DROPOUT 2 ------\n",
            "------- ADD AND LAYER NORMALIZATION 2 ------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn( (batch_size, max_sequence_length, d_model) ) # includes positional encoding\n",
        "out = encoder(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Regularization in Transformer Neural Networks**\n",
        "\n",
        "Regularization is a set of techniques used in neural networks, including Transformers, to prevent overfitting and improve generalization to unseen data. In Transformers, regularization is crucial due to their large number of parameters and the potential for overfitting on limited data. Here are the key regularization techniques commonly employed in Transformers:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Dropout**\n",
        "- **What it is:** A simple and widely-used technique where a fraction of the neurons in a layer are randomly \"dropped\" (set to zero) during training. This prevents the model from relying too heavily on specific neurons.\n",
        "- **Where it's applied in Transformers:**\n",
        "  - After the attention mechanism to regularize the learned attention weights.\n",
        "  - In the feedforward network to reduce over-reliance on specific feature transformations.\n",
        "  - Before or after residual connections to prevent co-adaptation of layers.\n",
        "- **How it helps:** Dropout reduces the risk of overfitting by encouraging the model to learn more robust features.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Attention Masking**\n",
        "- **What it is:** While not a traditional regularization method, masking (e.g., padding or future masking) controls the range of attention.\n",
        "- **Purpose:**\n",
        "  - In **sequence-to-sequence models**, masks prevent attention to future tokens (causal masking) in tasks like language modeling.\n",
        "  - Padding masks ensure the model ignores padded parts of the sequence.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Weight Decay (L2 Regularization)**\n",
        "- **What it is:** Adds a penalty to the loss function based on the magnitude of the model's weights.\n",
        "  \n",
        "- **How it helps:** Encourages smaller weights, leading to simpler models that are less prone to overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Layer Normalization**\n",
        "- **What it is:** Normalizes the activations across the features in a layer to stabilize training and prevent exploding or vanishing gradients.\n",
        "- **In Transformers:** It is applied after the attention and feedforward sub-layers.\n",
        "- **How it helps:** By reducing internal covariate shifts, LayerNorm makes the model more robust to variations in input distributions.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Label Smoothing**\n",
        "- **What it is:** During training, instead of using one-hot encoded labels (where the correct label has a probability of 1), a small probability is assigned to all incorrect labels.\n",
        "\n",
        "- **How it helps:** Reduces the model's confidence in predictions, preventing it from becoming overly confident and overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Early Stopping**\n",
        "- **What it is:** Stops training when the model's performance on validation data stops improving.\n",
        "- **How it helps:** Prevents overfitting to the training data by halting training before the model starts to memorize the data.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Data Augmentation**\n",
        "- **What it is:** Modifies the training data to simulate diverse scenarios, such as adding noise, shuffling, or masking tokens (e.g., in BERT).\n",
        "- **In Transformers:** Techniques like token masking, token swapping, or span masking are used to encourage the model to generalize better.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Gradient Clipping**\n",
        "- **What it is:** Restricts the maximum norm of the gradients during backpropagation.\n",
        "- **How it helps:** Prevents exploding gradients, especially in attention-heavy architectures where gradients can become very large.\n",
        "\n",
        "---\n",
        "\n",
        "### **9. Knowledge Distillation**\n",
        "- **What it is:** A pre-trained \"teacher\" model guides a smaller \"student\" model by transferring knowledge.\n",
        "- **How it helps:** Enables the student model to generalize better by mimicking the teacher's predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### **10. Regularization in Residual Connections**\n",
        "- Transformers rely heavily on residual connections, which can introduce overfitting if left unchecked. Applying dropout or other techniques at these points ensures better regularization.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Regularization in Transformers combines multiple techniques, such as dropout, weight decay, label smoothing, and data augmentation, to combat overfitting and stabilize training. These methods are essential for scaling Transformers to large datasets and complex tasks while maintaining their generalization ability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwOOsnedJwaa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "TF_PT_Rec_System",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
